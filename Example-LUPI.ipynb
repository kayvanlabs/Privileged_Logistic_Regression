{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privileged Logistic Regression and Logistic Regression on Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the models\n",
    "from privileged_lr import PrivilegedLogisticRegression\n",
    "from cvxpy_lr import CvxpyLogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the Data for Learning Using Priviledged Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_informative = 12, 6\n",
    "\n",
    "# create a simluated dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=n_total, \n",
    "                           n_informative=n_informative, \n",
    "                           n_redundant=0, random_state=0)\n",
    "\n",
    "# split the dataset into train, validation and test set based on the ratio\n",
    "train_ratio, validation_ratio, test_ratio = 0.4, 0.3, 0.3\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=(1-train_ratio), random_state=0)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train, y_train, test_size=test_ratio/(test_ratio+validation_ratio), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=validation_ratio/(train_ratio+validation_ratio), random_state=1)\n",
    "\n",
    "# select out all the informtive columns as privileged information\n",
    "x_train_star = X_train[:, :(n_informative)]\n",
    "\n",
    "# the rest are used as base features\n",
    "x_train = X_train[:, (n_informative):]\n",
    "\n",
    "# in the val/test set, we only keep the base features\n",
    "x_val = X_val[:, n_informative:]\n",
    "x_test = X_test[:, n_informative:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running with PLR Model (`cvxpy` implementation) - Training, Hyper-parameter Selection and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Using Privliged Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hyperparameters searching grid\n",
    "param_grid_plr = {\n",
    "    'lambda_base': [0.01, 0.1, 1, 10],\n",
    "    'lambda_star': [0.01, 0.1, 1, 10],\n",
    "    'alpha': [0.01, 0.1, 1, 10],\n",
    "    'xi_link': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1']\n",
    "    }\n",
    "all_hyperparam_combinations = list(itertools.product(*map(param_grid_plr.get, list(param_grid_plr))))\n",
    "# initialize the dataframes to store the results\n",
    "df_train_plr = pd.DataFrame(columns=['lambda_base', 'lambda_star', 'alpha', 'xi_link', 'penalty', 'auroc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for each hyperparameter combination and iterate over it\n",
    "for i, hyper_param_values in enumerate(all_hyperparam_combinations):\n",
    "    kwarg = dict(zip(list(param_grid_plr.keys()), hyper_param_values))\n",
    "\n",
    "    # initialize the model with the hyperparameters\n",
    "    plr_model = PrivilegedLogisticRegression(**kwarg)\n",
    "    \n",
    "    # fit the plr_model\n",
    "    plr_model.fit(x_train, y_train, X_star=x_train_star, y_star=y_train)\n",
    "\n",
    "    # obtain the prediction\n",
    "    y_val_pred = plr_model.predict_proba(x_val)\n",
    "\n",
    "    # calculate the AUROC\n",
    "    auroc = roc_auc_score(y_val, y_val_pred[:, 1])\n",
    "    f1 = f1_score(y_val, y_val_pred.argmax(axis=1))\n",
    "    # store the validation results\n",
    "    df_train_plr.loc[i] = list(hyper_param_values) + [auroc, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PLR model) AUROC on test set: 0.8661017137646104\n",
      "(PLR model) Accuracy on test set: 0.7933333333333333\n",
      "(PLR model) F1 score on test set: 0.8154761904761906\n",
      "(PLR model) Precision on test set: 0.7548209366391184\n",
      "(PLR model) Recall on test set: 0.8867313915857605\n"
     ]
    }
   ],
   "source": [
    "# obtain the best hyperparameters\n",
    "best_hyperparam = df_train_plr.sort_values(by='f1', ascending=False).iloc[1] \n",
    "\n",
    "# only keep the best hyperparameters in param_grid.keys()\n",
    "best_hyperparam = best_hyperparam[list(param_grid_plr.keys())]\n",
    "\n",
    "# apply the best hyperparameters to the best_plr_model\n",
    "best_plr_model = PrivilegedLogisticRegression(**best_hyperparam.to_dict())\n",
    "\n",
    "# fit the best_plr_model\n",
    "best_plr_model.fit(x_train, y_train, X_star=x_train_star, y_star=y_train)\n",
    "\n",
    "# obtain the prediction\n",
    "y_test_pred = best_plr_model.predict_proba(x_test)\n",
    "\n",
    "# calculate the AUROC, accuracy, f1, precision and recall\n",
    "auroc = roc_auc_score(y_test, y_test_pred[:, 1])\n",
    "acc = accuracy_score(y_test, y_test_pred.argmax(axis=1))\n",
    "f1 = f1_score(y_test, y_test_pred.argmax(axis=1))\n",
    "precision = precision_score(y_test, y_test_pred.argmax(axis=1))\n",
    "recall = recall_score(y_test, y_test_pred.argmax(axis=1))\n",
    "\n",
    "print('(PLR model) AUROC on test set: {}'.format(auroc))\n",
    "print('(PLR model) Accuracy on test set: {}'.format(acc))\n",
    "print('(PLR model) F1 score on test set: {}'.format(f1))\n",
    "print('(PLR model) Precision on test set: {}'.format(precision))\n",
    "print('(PLR model) Recall on test set: {}'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LR Model (`sklearn` implementation) - Training, Hyper-parameter Selection and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical Learning Paradigm w/o Privileged Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LR model sklearn) AUROC on test set: 0.861352995473704\n",
      "(LR model sklearn) Accuracy on test set: 0.795\n",
      "(LR model sklearn) F1 score on test set: 0.8025682182985555\n",
      "(LR model sklearn) Precision on test set: 0.7961783439490446\n",
      "(LR model sklearn) Recall on test set: 0.8090614886731392\n"
     ]
    }
   ],
   "source": [
    "# initialize the hyperparameters searching grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1']\n",
    "    }\n",
    "all_hyperparam_combinations = list(itertools.product(*map(param_grid.get, list(param_grid))))\n",
    "# initialize the dataframes to store the results\n",
    "df_train = pd.DataFrame(columns=['C', 'penalty', 'auroc', 'f1'])\n",
    "\n",
    "# create a dictionary for each hyperparameter combination and iterate over it\n",
    "for i, hyper_param_values in enumerate(all_hyperparam_combinations):\n",
    "    kwarg = dict(zip(list(param_grid.keys()), hyper_param_values))\n",
    "\n",
    "    # initialize the model with the hyperparameters\n",
    "    lr_model = LogisticRegression(solver='saga', **kwarg)\n",
    "\n",
    "    # fit the lr_model\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    \n",
    "    # obtain the prediction\n",
    "    y_val_pred = lr_model.predict_proba(x_val)\n",
    "\n",
    "    # calculate the AUROC\n",
    "    auroc = roc_auc_score(y_val, y_val_pred[:, 1])\n",
    "    f1 = f1_score(y_val, y_val_pred.argmax(axis=1))\n",
    "\n",
    "    # store the validation results\n",
    "    df_train.loc[i] = list(hyper_param_values) + [auroc, f1]\n",
    "\n",
    "# obtain the best hyperparameters\n",
    "best_hyperparam = df_train.sort_values(by='f1', ascending=False).iloc[0]\n",
    "\n",
    "\n",
    "# only keep the best hyperparameters in param_grid.keys()\n",
    "best_hyperparam = best_hyperparam[list(param_grid.keys())]\n",
    "\n",
    "# apply the best hyperparameters to the best_lr_model\n",
    "best_lr_model = LogisticRegression(solver='saga', **best_hyperparam.to_dict())\n",
    "\n",
    "# fit the best_lr_model\n",
    "best_lr_model.fit(x_train, y_train)\n",
    "\n",
    "# obtain the prediction\n",
    "y_test_pred = best_lr_model.predict_proba(x_test)\n",
    "\n",
    "# calculate the AUROC, accuracy, f1, precision and recall\n",
    "auroc = roc_auc_score(y_test, y_test_pred[:, 1])\n",
    "acc = accuracy_score(y_test, y_test_pred.argmax(axis=1))\n",
    "f1 = f1_score(y_test, y_test_pred.argmax(axis=1))\n",
    "precision = precision_score(y_test, y_test_pred.argmax(axis=1))\n",
    "recall = recall_score(y_test, y_test_pred.argmax(axis=1))\n",
    "\n",
    "print('(LR model sklearn) AUROC on test set: {}'.format(auroc))\n",
    "print('(LR model sklearn) Accuracy on test set: {}'.format(acc))\n",
    "print('(LR model sklearn) F1 score on test set: {}'.format(f1))\n",
    "print('(LR model sklearn) Precision on test set: {}'.format(precision))\n",
    "print('(LR model sklearn) Recall on test set: {}'.format(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LR Model (`cvxpy` implementation) - Training, Hyper-parameter Selection and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical Learning Paradigm w/o Privileged Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LR model cvxpy) AUROC on test set: 0.8655234155184112\n",
      "(LR model cvxpy) Accuracy on test set: 0.795\n",
      "(LR model cvxpy) F1 score on test set: 0.8098918083462133\n",
      "(LR model cvxpy) Precision on test set: 0.7751479289940828\n",
      "(LR model cvxpy) Recall on test set: 0.8478964401294499\n"
     ]
    }
   ],
   "source": [
    "# initialize the hyperparameters searching grid\n",
    "param_grid = {\n",
    "    'lambda_': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1']\n",
    "    }\n",
    "all_hyperparam_combinations = list(itertools.product(*map(param_grid.get, list(param_grid))))\n",
    "# initialize the dataframes to store the results\n",
    "df_train = pd.DataFrame(columns=['lambda_', 'penalty', 'auroc', 'f1'])\n",
    "\n",
    "# create a dictionary for each hyperparameter combination and iterate over it\n",
    "for i, hyper_param_values in enumerate(all_hyperparam_combinations):\n",
    "    kwarg = dict(zip(list(param_grid.keys()), hyper_param_values))\n",
    "\n",
    "    # initialize the model with the hyperparameters\n",
    "    lr_model = CvxpyLogisticRegression(**kwarg)\n",
    "\n",
    "    # fit the lr_model\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    \n",
    "    # obtain the prediction\n",
    "    y_val_pred = lr_model.predict_proba(x_val)\n",
    "\n",
    "    # calculate the AUROC\n",
    "    auroc = roc_auc_score(y_val, y_val_pred[:, 1])\n",
    "    f1 = f1_score(y_val, y_val_pred.argmax(axis=1))\n",
    "\n",
    "    # store the validation results\n",
    "    df_train.loc[i] = list(hyper_param_values) + [auroc, f1]\n",
    "\n",
    "# obtain the best hyperparameters\n",
    "best_hyperparam = df_train.sort_values(by='f1', ascending=False).iloc[0]\n",
    "\n",
    "# only keep the best hyperparameters in param_grid.keys()\n",
    "best_hyperparam = best_hyperparam[list(param_grid.keys())]\n",
    "\n",
    "# apply the best hyperparameters to the best_lr_model\n",
    "best_lr_model = CvxpyLogisticRegression(**best_hyperparam.to_dict())\n",
    "\n",
    "# fit the best_lr_model\n",
    "best_lr_model.fit(x_train, y_train)\n",
    "\n",
    "# obtain the prediction\n",
    "y_test_pred = best_lr_model.predict_proba(x_test)\n",
    "\n",
    "# calculate the AUROC, accuracy, f1, precision and recall\n",
    "auroc = roc_auc_score(y_test, y_test_pred[:, 1])\n",
    "acc = accuracy_score(y_test, y_test_pred.argmax(axis=1))\n",
    "f1 = f1_score(y_test, y_test_pred.argmax(axis=1))\n",
    "precision = precision_score(y_test, y_test_pred.argmax(axis=1))\n",
    "recall = recall_score(y_test, y_test_pred.argmax(axis=1))\n",
    "\n",
    "print('(LR model cvxpy) AUROC on test set: {}'.format(auroc))\n",
    "print('(LR model cvxpy) Accuracy on test set: {}'.format(acc))\n",
    "print('(LR model cvxpy) F1 score on test set: {}'.format(f1))\n",
    "print('(LR model cvxpy) Precision on test set: {}'.format(precision))\n",
    "print('(LR model cvxpy) Recall on test set: {}'.format(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
