{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privileged Logistic Regression with Partial Availabile Privileged Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the privileged logistic model with partially available privileged information cases, it's essential to prepare the data in such a way that the data rows that have privileged information available always come first and match with the base information. The following example illustrates how to prepare the data for such cases and how to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the models\n",
    "from privileged_lr import PrivilegedLogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the Data for Learning Using Partially Availabile Priviledged Information (LUPAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_informative = 12, 6\n",
    "\n",
    "# create a simluated dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=n_total, \n",
    "                           n_informative=n_informative, \n",
    "                           n_redundant=0, random_state=0)\n",
    "\n",
    "# split the dataset into train, validation and test set based on the ratio\n",
    "train_ratio, validation_ratio, test_ratio = 0.4, 0.3, 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=validation_ratio/(train_ratio+validation_ratio), random_state=1)\n",
    "\n",
    "# select out all the informtive columns as privileged information\n",
    "x_train_star = X_train[:, :(n_informative)]\n",
    "\n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "# randomly select 80% of the rows in privileged information to be available\n",
    "idx = np.random.choice(x_train_star.shape[0], int(x_train_star.shape[0]*0.8), replace=False)\n",
    "# identify the unselected idx\n",
    "no_pi_idx = np.setdiff1d(np.arange(x_train_star.shape[0]), idx)\n",
    "\n",
    "# only keep the selected rows in the privileged data and the label\n",
    "x_train_star = x_train_star[idx]\n",
    "y_train_star = y_train[idx]\n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "\n",
    "# the rest are used as base features\n",
    "x_train = X_train[:, (n_informative):]\n",
    "\n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "# Reaggregate the training set to keep consistency with privileged training data,\n",
    "# make sure to have idx first and then no_pi_idx\n",
    "x_train = np.concatenate((x_train[idx], x_train[no_pi_idx]), axis=0) \n",
    "y_train = np.concatenate((y_train[idx], y_train[no_pi_idx]), axis=0) \n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "\n",
    "# in the val/test set, we only keep the base features\n",
    "x_val = X_val[:, n_informative:]\n",
    "x_test = X_test[:, n_informative:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running with PLR Model (`cvxpy` implementation) - Training, Hyper-parameter Selection and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Using Partially Availabile Priviledged Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hyperparameters searching grid\n",
    "param_grid_plr = {\n",
    "    'lambda_base': [0.01, 0.1, 1, 10],\n",
    "    'lambda_star': [0.01, 0.1, 1, 10],\n",
    "    'alpha': [0.01, 0.1, 1, 10],\n",
    "    'xi_link': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1']\n",
    "    }\n",
    "all_hyperparam_combinations = list(itertools.product(*map(param_grid_plr.get, list(param_grid_plr))))\n",
    "# initialize the dataframes to store the results\n",
    "df_train_plr = pd.DataFrame(columns=['lambda_base', 'lambda_star', 'alpha', 'xi_link', 'penalty', 'auroc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for each hyperparameter combination and iterate over it\n",
    "for i, hyper_param_values in enumerate(all_hyperparam_combinations):\n",
    "    kwarg = dict(zip(list(param_grid_plr.keys()), hyper_param_values))\n",
    "\n",
    "    # initialize the model with the hyperparameters\n",
    "    plr_model = PrivilegedLogisticRegression(**kwarg)\n",
    "    \n",
    "    # If the data is sparse and the hyperparameter combination is not valid,\n",
    "    # the model will raise an error. We can catch the error and skip the \n",
    "    # hyperparameter combination.\n",
    "    # ---------------------------- LUPAPI ---------------------------- #\n",
    "    plr_model.fit(x_train, y_train, \n",
    "                  X_star=x_train_star, \n",
    "                  y_star=y_train_star) # <= be sure to use the privileged label here in LUPAPI case\n",
    "    # ---------------------------- LUPAPI ---------------------------- #\n",
    "\n",
    "    # obtain the prediction\n",
    "    y_val_pred = plr_model.predict_proba(x_val)\n",
    "\n",
    "    # calculate the AUROC\n",
    "    auroc = roc_auc_score(y_val, y_val_pred[:, 1])\n",
    "    f1 = f1_score(y_val, y_val_pred.argmax(axis=1))\n",
    "    # store the validation results\n",
    "    df_train_plr.loc[i] = list(hyper_param_values) + [auroc, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PLR model) AUROC on test set: 0.8721985799842221\n",
      "(PLR model) Accuracy on test set: 0.795\n",
      "(PLR model) F1 score on test set: 0.7946577629382304\n",
      "(PLR model) Precision on test set: 0.7933333333333333\n",
      "(PLR model) Recall on test set: 0.7959866220735786\n"
     ]
    }
   ],
   "source": [
    "# obtain the best hyperparameters\n",
    "best_hyperparam = df_train_plr.sort_values(by='f1', ascending=False).iloc[0] \n",
    "\n",
    "# only keep the best hyperparameters in param_grid.keys()\n",
    "best_hyperparam = best_hyperparam[list(param_grid_plr.keys())]\n",
    "\n",
    "# apply the best hyperparameters to the best_plr_model\n",
    "best_plr_model = PrivilegedLogisticRegression(**best_hyperparam.to_dict())\n",
    "\n",
    "# fit the best_plr_model\n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "best_plr_model.fit(x_train, y_train, \n",
    "                   X_star=x_train_star, \n",
    "                   y_star=y_train_star) # <= be sure to use the privileged label here in LUPAPI case\n",
    "# ---------------------------- LUPAPI ---------------------------- #\n",
    "\n",
    "# obtain the prediction\n",
    "y_test_pred = best_plr_model.predict_proba(x_test)\n",
    "\n",
    "# calculate the AUROC, accuracy, f1, precision and recall\n",
    "auroc = roc_auc_score(y_test, y_test_pred[:, 1])\n",
    "acc = accuracy_score(y_test, y_test_pred.argmax(axis=1))\n",
    "f1 = f1_score(y_test, y_test_pred.argmax(axis=1))\n",
    "precision = precision_score(y_test, y_test_pred.argmax(axis=1))\n",
    "recall = recall_score(y_test, y_test_pred.argmax(axis=1))\n",
    "\n",
    "print('(PLR model) AUROC on test set: {}'.format(auroc))\n",
    "print('(PLR model) Accuracy on test set: {}'.format(acc))\n",
    "print('(PLR model) F1 score on test set: {}'.format(f1))\n",
    "print('(PLR model) Precision on test set: {}'.format(precision))\n",
    "print('(PLR model) Recall on test set: {}'.format(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
